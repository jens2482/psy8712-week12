---
title: "PSY 8712 Week 12 Project"
author: "Amanda Jensen"
output: html_document
---

### Script Settings and Resources
```{r script settings and resources}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)      
library(RedditExtractoR)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(topicmodels)
library(tidytext)
```

### Data Import and Cleaning
```{r create initial scraped dataset}
#import content from reddit - went with Reddit Extractor after previous web scraping project
# reddit_thread_urls <- find_thread_urls(
#   subreddit = "IOPsychology",
#   sort_by = "new",
#   period = "year")

#pull out data we want from reddit extraction
# reddit_content <- get_thread_content(reddit_thread_urls$url)

#create csv file of urls
# write_csv(reddit_thread_urls, "../data/reddit_thread_urls.csv")

#create csv file from the scraped reddit data
# write_csv(reddit_content$threads, "../data/reddit_content.csv")

#import data from csv file
data <- read_csv("../data/reddit_content.csv")

#pull necessary columns into tibble
week12_tbl <- tibble(
  upvotes = data$upvotes,
  title = data$title
)
#add doc id - since in the previous document you only wanted two variables, I followed the directions. However, the doc id is needed in the final tibble so I found it was easiest to include it at the start.
week12_tbl_id <-  week12_tbl %>% mutate(doc_id = row_number())
```

```{r create corpus and clean}
io_corpus_original <- VCorpus(VectorSource(week12_tbl_id$title))

#create customized function to check pre-processing - used code from class lecture
compare_them <- function() {
  casenum <- sample(1:952, 1) 
  print(io_corpus_original[[casenum]]$content)
  print(io_corpus[[casenum]]$content)
}

#create customized cleaning function - this is how they did it in data camp (though I know it's not really necessary to create a function since we're only applying it to one corpus)
clean_corpus <- function(corpus) {
  corpus <- corpus %>%
    tm_map(content_transformer(replace_abbreviation)) %>% #get rid of abbreviations
    tm_map(content_transformer(replace_contraction)) %>% #replace contractions
    tm_map(removePunctuation) %>%   #remove punctuation
    tm_map(content_transformer(tolower)) %>%  #make lowercase 
    tm_map(removeWords, words = c(stopwords("en"), "io psychology", "io psych", "io", "riopsychology", "iopsychology", "iopsych")) %>%  #remove stop words
    tm_map(stripWhitespace) %>%  #strip whitespace
    tm_map(content_transformer(lemmatize_words))
  return(corpus)
}

#apply cleaning function to corpus
io_corpus <- clean_corpus(io_corpus_original)

#run function comparing cleaned corpus to original
compare_them() 
```

```{r create dtm}
#set up bigram tokenizer
bigram_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=2))

#create dtm
io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = bigram_tokenizer))

#view dtm
io_dtm %>%
  as.matrix %>% 
  as_tibble %>%
  View

#remove sparse terms
io_slim_dtm <- removeSparseTerms(io_dtm, .997) #gives a N/k ratio of 2.17
```

```{r run lda and final tibble}
#get rid of rows with all zeros - N/k ratio is still 2.04 - Code won't run with rows with zeros
io_slim_dtm2 <- io_slim_dtm[rowSums(as.matrix(io_slim_dtm)) > 0, ]

#determine number of topics to extract
dtm_tune <- FindTopicsNumber(
  io_slim_dtm2,
  topics = seq(2,10,1), #test out between 2 and 10 topics 1 at a time
  metrics = c(       #metrics used in class code
    "Griffiths2004",
    "CaoJuan2009",
    "Arun2010",
    "Deveaud2014"),
  verbose = T
)

#plot results of models to determine number of topics - 4 or 5
FindTopicsNumber_plot(dtm_tune)

#run lda topic modeling - runs the lda and the displays the betas in a matrix
lda_results <- LDA(
  io_slim_dtm2, 
  k = 4, 		#try 4 topics
  method = "Gibbs",
  control = list(seed = 42)
)

lda_beta <- lda_results %>%
  tidy(matrix = "beta") 

lda_gamma <- lda_results %>%
  tidy(matrix = "gamma") %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(document = as.numeric(document)) %>%
  arrange(document) 

#pull data needed for final tibble
docid_name <- tibble(
  title = data$title
) %>%
  mutate(document = row_number())

topics_tbl <- lda_gamma %>%
  left_join (docid_name, by = "document") %>%
  rename (doc_id = document, probability = gamma, original = title) %>%
  select (doc_id,original, topic, probability) %>%
  as_tibble()
```

### Visualization

### Analysis

### Publication


