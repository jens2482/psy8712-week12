---
title: "PSY 8712 Week 12 Project"
author: "Amanda Jensen"
output: html_document
---

### Script Settings and Resources
```{r script settings and resources}
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)      
library(RedditExtractoR)
library(tm)
library(qdap)
library(textstem)
library(RWeka)
library(ldatuning)
library(topicmodels)
library(tidytext)
library(wordcloud)
library(haven)
library(caret)
```

### Data Import and Cleaning
```{r create initial scraped dataset}
#import content from reddit - went with Reddit Extractor after previous web scraping project
# reddit_thread_urls <- find_thread_urls(
#   subreddit = "IOPsychology",
#   sort_by = "new",
#   period = "year")

#pull out data we want from reddit extraction
# reddit_content <- get_thread_content(reddit_thread_urls$url)

#create csv file of urls
# write_csv(reddit_thread_urls, "../data/reddit_thread_urls.csv")

#create csv file from the scraped reddit data
# write_csv(reddit_content$threads, "../data/reddit_content.csv")

#import data from csv file
data <- read_csv("../data/reddit_content.csv")

#pull necessary columns into tibble
week12_tbl <- tibble(
  upvotes = data$upvotes,
  title = data$title
)
```

```{r create corpus and clean}
io_corpus_original <- VCorpus(VectorSource(week12_tbl$title))

#create customized function to check pre-processing - used code from class lecture
compare_them <- function() {
  casenum <- sample(1:952, 1) 
  print(io_corpus_original[[casenum]]$content)
  print(io_corpus[[casenum]]$content)
}

#create customized cleaning function - this is how they did it in data camp (though I know it's not really necessary to create a function since we're only applying it to one corpus)
clean_corpus <- function(corpus) {
  corpus <- corpus %>%
    tm_map(content_transformer(replace_abbreviation)) %>% #get rid of abbreviations
    tm_map(content_transformer(replace_contraction)) %>% #replace contractions
    tm_map(removePunctuation) %>%   #remove punctuation
    tm_map(content_transformer(tolower)) %>%  #make lowercase 
    tm_map(removeWords, words = c(stopwords("en"), "io psychology", "io psych", "io", "riopsychology", "iopsychology", "iopsych")) %>%  #remove stop words
    tm_map(stripWhitespace) %>%  #strip whitespace
    tm_map(content_transformer(lemmatize_words))
  return(corpus)
}

#apply cleaning function to corpus
io_corpus <- clean_corpus(io_corpus_original)

#run function comparing cleaned corpus to original
compare_them() 
```

```{r create dtm}
#set up bigram tokenizer
bigram_tokenizer <- function(x) NGramTokenizer(x, Weka_control(min=1, max=2))

#create dtm with bigrams
io_dtm <- DocumentTermMatrix(io_corpus, control = list(tokenize = bigram_tokenizer))

#view dtm as tibble - I use this later to create the word cloud too
io_dtm_tbl <- io_dtm %>%
  as.matrix %>% 
  as_tibble 

#remove sparse terms
io_slim_dtm <- removeSparseTerms(io_dtm, .997) #gives a N/k ratio of 2.17

#get rid of rows with all zeros - N/k ratio is still 2.04 - Code won't run with rows with zeros
io_slim_dtm2 <- io_slim_dtm[rowSums(as.matrix(io_slim_dtm)) > 0, ]
```

```{r final tbl creation}
final_tbl <- topics_tbl %>%
  left_join(week12_tbl, by = c("original" = "title"))
```


### Analysis
```{r run lda and final tibble}
#determine number of topics to extract
dtm_tune <- FindTopicsNumber(
  io_slim_dtm2,
  topics = seq(2,10,1), #test out between 2 and 10 topics 1 at a time
  metrics = c(       #metrics used in class code
    "Griffiths2004",
    "CaoJuan2009",
    "Arun2010",
    "Deveaud2014"),
  verbose = T
)

#plot results of models to determine number of topics - 4 or 5
FindTopicsNumber_plot(dtm_tune)

#run lda topic modeling - runs the lda and the displays the betas in a matrix
lda_results <- LDA(
  io_slim_dtm2, 
  k = 4, 		#try 4 topics
  method = "Gibbs",
  control = list(seed = 42)
)

#look at betas (probability a term belongs in a topic)
lda_beta <- lda_results %>% 
  tidy(matrix = "beta") 

#look at gammas (probability a document belongs to a topic)
lda_gamma <- lda_results %>%
  tidy(matrix = "gamma") %>%
  group_by(document) %>%
  top_n(1, gamma) %>%
  slice(1) %>%
  ungroup %>%
  mutate(document = as.numeric(document)) %>%
  arrange(document) 

#pull data needed for final tibble
docid_name <- tibble(
  title = data$title
) %>%
  mutate(document = row_number())

#total tibble
topics_tbl <- lda_gamma %>%
  left_join (docid_name, by = "document") %>% #get titles from docid_name
  rename (doc_id = document, probability = gamma, original = title) %>% #change headings to match assignment
  select (doc_id,original, topic, probability) %>% #reorder to match assignment
  as_tibble() #is this necessary? 

##### Using the beta matrix alone, what topics would you conclude your final topic list maps onto? (e.g., topic 1, 2, 3…n each reflect what substantive topic construct? Use your best judgment.)
##### I think that topic 1 would be representing academic or scholarly thinking. The words with the highest betas were think, discussion, and reading, so I think this topic would be something about discussing things or thinking academically. I think topic 2 has to do with post-graduate opportunities. The top words were work, psychology, masters, advice, etc. All these words have to do with life after the program ends. For topic 3 it's a little tricky because the words are similar to topic 2, but I think it might be more of a career advice topic. Words like job, career, and help are the top words so this might be more people who are already graduated looking for advice in their job maybe. This one is tough. Topic 4 seems like it might be the most general, but perhaps a topic could be research aid. Words like research, looking, anyone, lead me to believe these are the types of topics where people are looking for people to participate in research.  

##### Look at the original text of documents with the highest and lowest probabilities assigned to each document. Do your topic names derived from your interpretation of the beta matrix conceptually match with the content of the original posts? What kind of validity evidence does your answer to this question represent?
##### It appears that topic 1 was pretty close. Many of the titles are related to "What have you been reading, and what do you think of it?", so it is people generating a discussion about current literature. Topic 2 I was pretty close on. It looks like a lot of the topics are asking for advice about what programs to join (rather than what to do post-grad) but the idea of seeking advice related to the program was still what I was thinking, just at the wrong point in time. Topic 3 was pretty close, that's more about career advice.Topic 4 appears to be sort of miscellaneous. The topics are related to random questions about certification, where people are staying for a conference, and other general inquiries, so I would say I didn't quite get that topic right. 
```

```{r statistic and machine learning analysis}
library(caret)

#set up train and test sets
rows <- sample(nrow(final_tbl)) # Shuffle row indices
shuffled_data <- final_tbl[rows,] # Put data in random order
split <- round(nrow(final_tbl) * 0.75) # Determine row to split on
train_tbl <- shuffled_data[1:split,] # Create train set
test_tbl <- shuffled_data[(split + 1):nrow(shuffled_data),] # Create test set

#training folds
training_folds <- createFolds(train_tbl$upvotes, k = 10, returnTrain = TRUE)

#statistical model - used same code from ML assignment
model1 <- train(
  upvotes ~ topic,
  train_tbl,
  method = "lm",
  na.action = na.pass,
  preProcess = c("center", "scale", "zv", "nzv", "medianImpute"),
  trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE, index = training_folds)
)
model1
cv_m1 <- model1$results$Rsquared

#metrics for holdout set - used same code from ML assignment
holdout_m1 <- cor(predict(model1, test_tbl, na.action = na.pass), test_tbl$upvotes)^2

#machine learning model (random forest) -  used same code from ML assignment
model2 <- train(
  upvotes ~ topic,
  data = train_tbl,
  method = "ranger",
  trControl = trainControl(method = "cv", number = 10, verboseIter = TRUE, index = training_folds),
  tuneGrid = data.frame(.mtry = 1, .splitrule = "variance", .min.node.size = 1),
  na.action = na.pass,
  preProcess = c("center", "scale", "zv", "nzv", "medianImpute")
)

#metrics for test set-  used same code from ML assignment
model2
cv_m2 <- max(model2$results$Rsquared)
holdout_m2 <- cor(predict(model2, test_tbl, na.action = na.pass), test_tbl$upvotes)^2

#summary of model comparisons
summary(resamples(list(model1 = model1, model2 = model2)), metric = "Rsquared")
dotplot(resamples(list(model1 = model1, model2 = model2)), metric = "Rsquared")

#In comparing how well a linear model and a random forest model predict upvotes, it turns out they perform pretty similarly under the best conditions, with the random forest just slightly ahead. Even at their best, though, both models can only explain about 15% of what influences upvotes. So there’s a lot going on that these models aren’t capturing and neither of them do a very good job.
```

### Visualization
```{r}
# Word Cloud - used class code
wordcloud(
  words = names(io_dtm_tbl),
  freq = colSums(io_dtm_tbl),
  colors = brewer.pal(9,"YlOrBr")
)

##### The word that stands out the largest is "job". That makes sense that on reddit the most people are interested in jobs, because that's ultimately what most people's goals probably are in entering and IO psychology program. A lot of the other words are the ones I highlighted for our topics, things like research masters, think, discussion, career, reading etc. So those most frequent terms dictated the topics. 
```